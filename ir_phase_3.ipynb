{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iL3mOh6IJeNl"
      },
      "source": [
        "<div style=\"direction:rtl;line-height:300%;\">\n",
        "<font face=\"XB Zar\" size=5>\n",
        "<div align=center>\n",
        "<font face=\"B Titr\" size=5>\n",
        "<p></p><p></p>\n",
        "بسمه تعالی\n",
        "<p></p>\n",
        "</font>\n",
        "<p></p>\n",
        "<font>\n",
        "<br>\n",
        "درس بازیابی پیشرفته اطلاعات\n",
        "<br>\n",
        "مدرس: دکتر سلیمانی\n",
        "</font>\n",
        "<p></p>\n",
        "<br>\n",
        "<font>\n",
        "<b>فاز سوم پروژه</b>\n",
        "</font>\n",
        "<br>\n",
        "<br>\n",
        "موعد تحویل:  ساعت ۶ صبح ۸ تیر<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<font>\n",
        "دانشگاه صنعتی شریف\n",
        "<br>\n",
        "دانشکده مهندسی کامپیوتر\n",
        "<br>\n",
        "<br>\n",
        "</font>\n",
        "</div>\n",
        "</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ps02VgwVJeNn"
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "<font face=\"XB Zar\" size=5>\n",
        "    <h1>\n",
        "    <b>مقدمه</b>\n",
        "    </h1>\n",
        "    <p></p>\n",
        "    <p></p>\n",
        "</font>\n",
        "<font face=\"XB Zar\" size=3>\n",
        "     <br>\n",
        "    در این فاز از پروژه، تمرکز ما بر\n",
        "    crawling\n",
        "    و تحلیل مقالات استخراج‌شده از اینترنت خواهد بود. ما با بررسی تکنیک های مختلف\n",
        "    web crawling\n",
        "    برای استخراج مقالات و سایر اطلاعات مرتبط از وب شروع خواهیم کرد.\n",
        "    <br>\n",
        "    در مرحله بعد، الگوریتم های تجزیه و تحلیل  لینک مانند\n",
        "    PageRank\n",
        "    و\n",
        "    HITS\n",
        "    را برای تعیین اهمیت این مقالات بر اساس نقل قول‌ها، ارجاعات یا اشکال دیگر پیوندها اعمال خواهیم‌کرد. ما همچنین یاد خواهیم‌گرفت که چگونه یک الگوریتم\n",
        "    PageRank\n",
        "    شخصی‌سازی‌شده را پیاده‌سازی کنیم که ترجیحات کاربر را برای ارائه نتایج مرتبط تر در نظر می‌گیرد.\n",
        "    <br>\n",
        "    در بخش سوم این مرحله، یک موتور جستجوی شخصی‌سازی شده را پیاده‌سازی میکنیم و یاد می‌گیریم که چگونه موتور جستجویی بسازیم که نتایجی را بر اساس ترجیحات کاربر ارائه دهد.\n",
        "    <br>\n",
        "در نهایت، ما یک\n",
        "    task\n",
        "     در مورد\n",
        "    recommendation system\n",
        "    ها خواهیم‌داشت، که در آن از تکنیک های مختلف برای توصیه مقالات یا صفحات وب به کاربران بر اساس ترجیحات و رفتار آنها استفاده خواهیم کرد.\n",
        "    <br>\n",
        "     تنها زبان قابل قبول برای پروژه پایتون است. محدودیت استفاده از کتاب‌خانه‌های آماده در هر بخش مشخص شده است. در انتهای پروژه قرار است یک سیستم یکپارچه‌ی جست‌و‌جو داشته باشید، بنابراین به پیاده‌سازی هر چه بهتر این فاز توجه داشته باشید.\n",
        "</font>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBCLUwHRJeNo"
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "<font face=\"XB Zar\" size=4>\n",
        "    <h1>\n",
        "    <b>پیاده‌سازی Crawler (۴۰ نمره)</b>\n",
        "    </h1>\n",
        "</font>\n",
        "    <br>\n",
        "\n",
        "<font face=\"XB Zar\" size=3>\n",
        "   در این بخش باید یک Crawler\n",
        "    برای واکشی اطلاعات تعدادی مقاله از سایت <a href=\"https://www.semanticscholar.org/\">Semantic Scholar</a> پیاده سازی کنید.\n",
        "   اطلاعات واکشی شده باید حاوی موارد زیر باشد.\n",
        "</font>\n",
        "</div>\n",
        "<br>\n",
        "<table dir=\"ltr\" style=\"width: 100%; border-collapse: collapse;\">\n",
        "  <tr>\n",
        "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">ID</th>\n",
        "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Title</th>\n",
        "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Abstract</th>\n",
        "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Publication Year</th>\n",
        "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Authors</th>\n",
        "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Related Topics</th>\n",
        "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Citation Count</th>\n",
        "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Reference Count</th>\n",
        "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">References</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Unique ID of the paper</td>\n",
        "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Title of the paper</td>\n",
        "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Abstract of the paper</td>\n",
        "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Publication year</td>\n",
        "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Name of the first author, ..., Name of the last author</td>\n",
        "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">topic1, topic2, ...</td>\n",
        "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">number of citations of the paper</td>\n",
        "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">number of references of the paper</td>\n",
        "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">ID of the first reference, ..., ID of the tenth reference</td>\n",
        "  </tr>\n",
        "</table>\n",
        "    <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdklK_NaJeNo"
      },
      "source": [
        "\n",
        "<div dir=\"rtl\">\n",
        "\n",
        "<font face=\"XB Zar\" size=3>\n",
        "  ابتدا فرایند واکشی را از ۵ مقاله‌ی هر استاد شروع کنید و\n",
        "    ۱۰\n",
        "    مرجع اول هر مقاله را به صف مقالات اضافه کنید.\n",
        "    فرایند واکشی را نا جایی ادامه دهید که اطلاعات ۲۰۰۰ مقاله را داشته باشید.\n",
        "    اطلاعات مقالات را در فایل crawled_paper_profName.json ذخیره کنید.\n",
        "</font>\n",
        "</div>\n",
        "\n",
        "<div dir=\"rtl\">\n",
        "\n",
        "<font face=\"XB Zar\" size=3>\n",
        "  در پیاده سازی Crawler به موارد زیر دقت کنید.\n",
        "\n",
        "\n",
        "<ul>\n",
        "<li>حق استفاده از api سایت semantic scholar را ندارید.</li>\n",
        "<li>برای واکشی می‌توانید از پکیج‌هایی مثل <a href=\"https://www.selenium.dev/selenium/docs/api/py/\">Selenium</a> و یا <a href=\"https://github.com/scrapy/scrapy\">Scrapy</a>  استفاده کنید. استفاده از پکیج‌های دیگر نیز مجاز است. همچنین برای پارس اطلاعات واکشی شده می‌توانید از پکیج <a href=\"https://pypi.org/project/beautifulsoup4/\">Beautiful Soup</a> استفاده کنید.\n",
        "</li>\n",
        "<li>بین هر بار درخواست از سایت یک فاصله چند ثانیه‌ای بدهید.</li>\n",
        "<li>در زمان تحویل کد Crawler شما اجرا خواهد شد و صحت آن بررسی خواهد شد.</li>\n",
        "<li>در صورتی که ‌Crawler شما به دچار اروری مثل request timeout شد نباید کار خود را متوقف کند.</li>\n",
        "</ul>\n",
        "\n",
        "\n",
        "</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDAuGQs23m87"
      },
      "outputs": [],
      "source": [
        "# For using selenium in colab\n",
        "\n",
        "\n",
        "%%shell\n",
        "# Ubuntu no longer distributes chromium-browser outside of snap\n",
        "#\n",
        "# Proposed solution: https://askubuntu.com/questions/1204571/how-to-install-chromium-without-snap\n",
        "\n",
        "# Add debian buster\n",
        "cat > /etc/apt/sources.list.d/debian.list <<'EOF'\n",
        "deb [arch=amd64 signed-by=/usr/share/keyrings/debian-buster.gpg] http://deb.debian.org/debian buster main\n",
        "deb [arch=amd64 signed-by=/usr/share/keyrings/debian-buster-updates.gpg] http://deb.debian.org/debian buster-updates main\n",
        "deb [arch=amd64 signed-by=/usr/share/keyrings/debian-security-buster.gpg] http://deb.debian.org/debian-security buster/updates main\n",
        "EOF\n",
        "\n",
        "# Add keys\n",
        "apt-key adv --keyserver keyserver.ubuntu.com --recv-keys DCC9EFBF77E11517\n",
        "apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 648ACFD622F3D138\n",
        "apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 112695A0E562B32A\n",
        "\n",
        "apt-key export 77E11517 | gpg --dearmour -o /usr/share/keyrings/debian-buster.gpg\n",
        "apt-key export 22F3D138 | gpg --dearmour -o /usr/share/keyrings/debian-buster-updates.gpg\n",
        "apt-key export E562B32A | gpg --dearmour -o /usr/share/keyrings/debian-security-buster.gpg\n",
        "\n",
        "# Prefer debian repo for chromium* packages only\n",
        "# Note the double-blank lines between entries\n",
        "cat > /etc/apt/preferences.d/chromium.pref << 'EOF'\n",
        "Package: *\n",
        "Pin: release a=eoan\n",
        "Pin-Priority: 500\n",
        "\n",
        "\n",
        "Package: *\n",
        "Pin: origin \"deb.debian.org\"\n",
        "Pin-Priority: 300\n",
        "\n",
        "\n",
        "Package: chromium*\n",
        "Pin: origin \"deb.debian.org\"\n",
        "Pin-Priority: 700\n",
        "EOF\n",
        "\n",
        "# Install chromium and chromium-driver\n",
        "apt-get update\n",
        "apt-get install chromium chromium-driver\n",
        "\n",
        "# Install selenium\n",
        "pip install selenium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XYZavuqJvUD"
      },
      "outputs": [],
      "source": [
        "!pip install beautifulsoup4\n",
        "!pip install lxml\n",
        "!pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fk8ysjUbodnM"
      },
      "outputs": [],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.common.by import By"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKqWQFIpjG_D"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import logging\n",
        "import json\n",
        "from random import randint\n",
        "from time import sleep\n",
        "from tqdm import tqdm\n",
        "import requests\n",
        "import re\n",
        "from collections import deque\n",
        "\n",
        "from typing import Dict, List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDXZXtDluW-Q",
        "outputId": "74bf8149-bca4-4bb4-c01e-0bc040b926fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at ./content; to attempt to forcibly remount, call drive.mount(\"./content\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('./content')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLowWILr7oZP"
      },
      "outputs": [],
      "source": [
        "def request_url(url):\n",
        "    global driver\n",
        "    driver.implicitly_wait(3)\n",
        "\n",
        "    try:\n",
        "      print(url)\n",
        "      driver.get(url)\n",
        "    except:\n",
        "      print(\"Page not loaded\")\n",
        "      return None\n",
        "\n",
        "    try:\n",
        "      try:\n",
        "        driver.find_element(By.XPATH, \"//button[@data-test-id='text-truncator-toggle']\").click()\n",
        "      except:\n",
        "        pass\n",
        "\n",
        "      driver.find_element(By.XPATH, \"//button[@data-test-id='author-list-expand']\").click()\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "    html = driver.page_source\n",
        "\n",
        "    return html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUVM14q5gCym"
      },
      "outputs": [],
      "source": [
        "def readfiles(prof, que):\n",
        "    with open('./content/MyDrive/paper_folder/'+prof) as file:\n",
        "        links = file.readlines()\n",
        "        for link in links:\n",
        "            link = link.rstrip('\\n')\n",
        "            que.append(link)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfVZtMzq_zQ3"
      },
      "outputs": [],
      "source": [
        "backup_papers = pd.DataFrame()\n",
        "papers_list = list()\n",
        "\n",
        "def scrape():\n",
        "    q = deque()\n",
        "\n",
        "    global papers_list\n",
        "\n",
        "    # read file of papers and add to q\n",
        "    readfiles('Soleymani.txt', q)\n",
        "    readfiles('Kasaei.txt', q)\n",
        "    readfiles('Rohban.txt', q)\n",
        "    readfiles('Sharifi.txt', q)\n",
        "    readfiles('Rabiee.txt', q)\n",
        "\n",
        "    while len(papers_list) < 2000  and len(q) > 1:\n",
        "        url = q.popleft()\n",
        "        links = extract_data(url, papers_list)\n",
        "\n",
        "        for link in links:\n",
        "            q.append(link)\n",
        "\n",
        "    global backup_papers\n",
        "    backup_papers = pd.DataFrame(papers_list)\n",
        "\n",
        "    # write to json file\n",
        "    with open('./crawled_paper_profName.json', 'w') as crawled:\n",
        "        json.dump(papers_list, crawled, indent=2)\n",
        "\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ha1L0t8-5pBz"
      },
      "outputs": [],
      "source": [
        "def handle_data(selector, **kwargs):\n",
        "    value = None\n",
        "    if kwargs.get('attrs') is None:\n",
        "        try:\n",
        "            value = selector.find(kwargs.get('tag')).text\n",
        "        except:\n",
        "            value = ''\n",
        "    else:\n",
        "        try:\n",
        "            value = selector.find(kwargs.get('tag'), attrs=kwargs.get('attrs')).text\n",
        "        except:\n",
        "            value = ''\n",
        "\n",
        "    return value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irvU9JKoM-BB"
      },
      "outputs": [],
      "source": [
        "def extract_data(url, papers_list):\n",
        "    papers = dict()\n",
        "\n",
        "    response = request_url(url)\n",
        "    if response is None:\n",
        "        print('Response not returned!')\n",
        "        return []\n",
        "\n",
        "    domain = \"https://www.semanticscholar.org\"\n",
        "\n",
        "    soup = BeautifulSoup(response, 'lxml')\n",
        "\n",
        "    paper = soup.find('div', class_='fresh-paper-detail-page__header')\n",
        "\n",
        "    corpus_id = handle_data(paper, tag='li', attrs={'data-test-id': 'corpus-id'})\n",
        "    if corpus_id != '':\n",
        "      corpus_id = int(re.findall(r'\\d+', corpus_id)[0])\n",
        "    else:\n",
        "      corpus_id = np.nan\n",
        "\n",
        "    papers['Corpus_ID'] = corpus_id\n",
        "    papers['ID'] = url[-10:]\n",
        "\n",
        "    papers['Title'] = handle_data(paper, tag='h1')\n",
        "    papers['Abstract'] =  handle_data(paper, tag='div', attrs={'data-test-id':'abstract-text'})\n",
        "    papers['Publication Year'] = handle_data(paper, tag='span', attrs={'data-test-id': 'paper-year'})\n",
        "\n",
        "    names_span = paper.find_all('span', attrs={'data-test-id': 'author-list'})\n",
        "    names_list = [name.text for name in names_span]\n",
        "    papers['Authors'] = ''.join(names_list)\n",
        "\n",
        "    try:\n",
        "      meta = paper.find_all('li', class_='paper-meta-item')\n",
        "      fields = meta[2].text\n",
        "    except:\n",
        "      fields = ''\n",
        "\n",
        "    papers['Related Topics'] = fields\n",
        "\n",
        "    page_nav = soup.find('div', class_='paper-nav')\n",
        "\n",
        "    # citation + refrence numbers\n",
        "    cite = handle_data(page_nav, tag='a', attrs={'data-heap-nav':'citing-papers'})\n",
        "    papers['Citation Count'] = int(re.findall(r'\\d+', cite)[0]) if cite != '' else np.nan\n",
        "\n",
        "    ref = handle_data(page_nav, tag='a', attrs={'data-heap-nav':'cited-papers'})\n",
        "    papers['Reference Count'] = int(re.findall(r'\\d+', ref)[0]) if ref != '' else np.nan\n",
        "\n",
        "    # finding references links\n",
        "    references = soup.find('div', class_='card cited-papers')\n",
        "    links = []\n",
        "    if references is None:\n",
        "      papers['References'] = []\n",
        "    else:\n",
        "      links_a = references.find_all('a', attrs={'data-heap-id': 'citation_title'})\n",
        "      links = [domain + link['href'] for link in links_a]\n",
        "\n",
        "      papers['References'] = [link[-10:] for link in links]     # id of reference papers is the last 10 char of url\n",
        "\n",
        "    papers_list.append(papers)\n",
        "\n",
        "    return links"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_9D2ZrdjFFE"
      },
      "outputs": [],
      "source": [
        "driver = None\n",
        "def crawl_setup():\n",
        "  global driver\n",
        "\n",
        "  options = Options()\n",
        "  options.add_argument(\"--headless\")\n",
        "  options.add_argument(\"--no-sandbox\")\n",
        "  options.headless = True\n",
        "\n",
        "  service = Service(executable_path=r'/usr/bin/chromedriver')\n",
        "  driver = webdriver.Chrome(service=service, options=options)\n",
        "\n",
        "  scrape()\n",
        "\n",
        "crawl_setup()\n",
        "driver.quit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7K4at_4UpBCW"
      },
      "outputs": [],
      "source": [
        "# papers = pd.DataFrame(papers_list)\n",
        "# papers\n",
        "\n",
        "# with open('./crawled_paper_profName.json', 'w') as crawled:\n",
        "#   json.dump(papers_list, crawled, indent=2)\n",
        "\n",
        "# with open('./content/MyDrive/paper_folder/crawled_paper_profName.json', 'w') as d:\n",
        "#   json.dump(papers_list, d, indent=2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        },
        "id": "dOP0T3jHtwlF",
        "outputId": "02968fa1-941d-4999-db42-390314c0b86a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-37ee4440-77e8-4313-874c-7c21536442fa\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Corpus_ID</th>\n",
              "      <th>ID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Abstract</th>\n",
              "      <th>Publication Year</th>\n",
              "      <th>Authors</th>\n",
              "      <th>Related Topics</th>\n",
              "      <th>Citation Count</th>\n",
              "      <th>Reference Count</th>\n",
              "      <th>References</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>232145932</td>\n",
              "      <td>78784ff663</td>\n",
              "      <td>Transformer-based deep neural network language...</td>\n",
              "      <td>Background We developed transformer-based deep...</td>\n",
              "      <td>9 March 2021</td>\n",
              "      <td>A. Roshanzamir, H. Aghajan, Mahdieh Soleymani ...</td>\n",
              "      <td>Computer Science</td>\n",
              "      <td>32.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>[4fb18a3560, ca52ece708, 902380a2c8, f62052d32...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>235097481</td>\n",
              "      <td>51cb6d1e30</td>\n",
              "      <td>MG-BERT: Multi-Graph Augmented BERT for Masked...</td>\n",
              "      <td>Pre-trained models like Bidirectional Encoder ...</td>\n",
              "      <td>1 June 2021</td>\n",
              "      <td>Parishad BehnamGhader, Hossein Zakerinia, Mahd...</td>\n",
              "      <td>Computer Science</td>\n",
              "      <td>1.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>[652660ddde, 1594e0e992, 215b4a9240, 704d2f26e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>196551931</td>\n",
              "      <td>611ae61e70</td>\n",
              "      <td>Deep Learning-Based Proarrhythmia Analysis Usi...</td>\n",
              "      <td>An early characterization of drug-induced card...</td>\n",
              "      <td>28 March 2019</td>\n",
              "      <td>Zeinab Golgooni, Sara Mirsadeghi, Mahdieh Sole...</td>\n",
              "      <td>Biology</td>\n",
              "      <td>8.0</td>\n",
              "      <td>46.0</td>\n",
              "      <td>[d42fcc494c, 7b3cc9650e, e33498b44d, a186029ed...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>231972684</td>\n",
              "      <td>1c3f9efb66</td>\n",
              "      <td>A Deep Learning Framework for Viable Tumor Bur...</td>\n",
              "      <td>Liver masses have become a common clinical cha...</td>\n",
              "      <td>23 December 2020</td>\n",
              "      <td>Seyed Alireza Fatemi Jahromi, Ali Asghar Khani...</td>\n",
              "      <td>Computer Science, Medicine</td>\n",
              "      <td>2.0</td>\n",
              "      <td>62.0</td>\n",
              "      <td>[8112adf21e, 7cb4a1caf7, b2e2f57a1f, 28742d657...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9801881</td>\n",
              "      <td>62b5d4d3fe</td>\n",
              "      <td>An attribute learning method for zero-shot rec...</td>\n",
              "      <td>Recently, the problem of integrating side info...</td>\n",
              "      <td>1 May 2017</td>\n",
              "      <td>Ramtin Yazdanian, Seyed Mohsen Shojaee, Mahdie...</td>\n",
              "      <td>Computer Science</td>\n",
              "      <td>NaN</td>\n",
              "      <td>29.0</td>\n",
              "      <td>[b24bc35766, e56eb14c9e, 06615485a2, d67363cd3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>3662132</td>\n",
              "      <td>8936ea1d26</td>\n",
              "      <td>Fast and accurate short read alignment with Bu...</td>\n",
              "      <td>Motivation: The enormous amount of short reads...</td>\n",
              "      <td>18 May 2009</td>\n",
              "      <td>Heng Li, R. Durbin</td>\n",
              "      <td>Computer Science</td>\n",
              "      <td>38.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>[c8849eb772, 352abe1fbc, e6415c4df3, 2c2f03071...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>8341349</td>\n",
              "      <td>7f12cb0b5d</td>\n",
              "      <td>High Throughput Short Read Alignment via Bi-di...</td>\n",
              "      <td>The advancement of sequencing technologies has...</td>\n",
              "      <td>1 November 2009</td>\n",
              "      <td>T. Lam, Ruiqiang Li, Alan Tam, Simon C. K. Won...</td>\n",
              "      <td>Computer Science</td>\n",
              "      <td>113.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>[8936ea1d26, cd5aee3fc8, c8849eb772, 124d5a1a7...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>16906100</td>\n",
              "      <td>7ee755c491</td>\n",
              "      <td>Fast and accurate long-read alignment with Bur...</td>\n",
              "      <td>Motivation: Many programs for aligning short s...</td>\n",
              "      <td>15 January 2010</td>\n",
              "      <td>Heng Li, R. Durbin</td>\n",
              "      <td>Computer Science</td>\n",
              "      <td>8.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>[8936ea1d26, f10d491a1a, 1c984a9afe, c8849eb77...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>14008339</td>\n",
              "      <td>130065f1f2</td>\n",
              "      <td>Space efficient linear time construction of su...</td>\n",
              "      <td></td>\n",
              "      <td>25 June 2003</td>\n",
              "      <td>P. Ko, S. Aluru</td>\n",
              "      <td>Computer Science</td>\n",
              "      <td>430.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>[4d75cbd1aa, 7150643504, 276859caf3, b8482011f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>8123356</td>\n",
              "      <td>024546c679</td>\n",
              "      <td>ABySS: a parallel assembler for short read seq...</td>\n",
              "      <td>Widespread adoption of massively parallel deox...</td>\n",
              "      <td>1 June 2009</td>\n",
              "      <td>J. Simpson, Kim Wong, S. Jackman, J. Schein, S...</td>\n",
              "      <td>Biology</td>\n",
              "      <td>3.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>[1526ccd81a, 124d5a1a7b, 773a726261, 58d000b90...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 10 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-37ee4440-77e8-4313-874c-7c21536442fa')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-37ee4440-77e8-4313-874c-7c21536442fa button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-37ee4440-77e8-4313-874c-7c21536442fa');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      Corpus_ID          ID  \\\n",
              "0     232145932  78784ff663   \n",
              "1     235097481  51cb6d1e30   \n",
              "2     196551931  611ae61e70   \n",
              "3     231972684  1c3f9efb66   \n",
              "4       9801881  62b5d4d3fe   \n",
              "...         ...         ...   \n",
              "1995    3662132  8936ea1d26   \n",
              "1996    8341349  7f12cb0b5d   \n",
              "1997   16906100  7ee755c491   \n",
              "1998   14008339  130065f1f2   \n",
              "1999    8123356  024546c679   \n",
              "\n",
              "                                                  Title  \\\n",
              "0     Transformer-based deep neural network language...   \n",
              "1     MG-BERT: Multi-Graph Augmented BERT for Masked...   \n",
              "2     Deep Learning-Based Proarrhythmia Analysis Usi...   \n",
              "3     A Deep Learning Framework for Viable Tumor Bur...   \n",
              "4     An attribute learning method for zero-shot rec...   \n",
              "...                                                 ...   \n",
              "1995  Fast and accurate short read alignment with Bu...   \n",
              "1996  High Throughput Short Read Alignment via Bi-di...   \n",
              "1997  Fast and accurate long-read alignment with Bur...   \n",
              "1998  Space efficient linear time construction of su...   \n",
              "1999  ABySS: a parallel assembler for short read seq...   \n",
              "\n",
              "                                               Abstract  Publication Year  \\\n",
              "0     Background We developed transformer-based deep...      9 March 2021   \n",
              "1     Pre-trained models like Bidirectional Encoder ...       1 June 2021   \n",
              "2     An early characterization of drug-induced card...     28 March 2019   \n",
              "3     Liver masses have become a common clinical cha...  23 December 2020   \n",
              "4     Recently, the problem of integrating side info...        1 May 2017   \n",
              "...                                                 ...               ...   \n",
              "1995  Motivation: The enormous amount of short reads...       18 May 2009   \n",
              "1996  The advancement of sequencing technologies has...   1 November 2009   \n",
              "1997  Motivation: Many programs for aligning short s...   15 January 2010   \n",
              "1998                                                         25 June 2003   \n",
              "1999  Widespread adoption of massively parallel deox...       1 June 2009   \n",
              "\n",
              "                                                Authors  \\\n",
              "0     A. Roshanzamir, H. Aghajan, Mahdieh Soleymani ...   \n",
              "1     Parishad BehnamGhader, Hossein Zakerinia, Mahd...   \n",
              "2     Zeinab Golgooni, Sara Mirsadeghi, Mahdieh Sole...   \n",
              "3     Seyed Alireza Fatemi Jahromi, Ali Asghar Khani...   \n",
              "4     Ramtin Yazdanian, Seyed Mohsen Shojaee, Mahdie...   \n",
              "...                                                 ...   \n",
              "1995                                 Heng Li, R. Durbin   \n",
              "1996  T. Lam, Ruiqiang Li, Alan Tam, Simon C. K. Won...   \n",
              "1997                                 Heng Li, R. Durbin   \n",
              "1998                                    P. Ko, S. Aluru   \n",
              "1999  J. Simpson, Kim Wong, S. Jackman, J. Schein, S...   \n",
              "\n",
              "                  Related Topics  Citation Count  Reference Count  \\\n",
              "0               Computer Science            32.0             54.0   \n",
              "1               Computer Science             1.0             29.0   \n",
              "2                        Biology             8.0             46.0   \n",
              "3     Computer Science, Medicine             2.0             62.0   \n",
              "4               Computer Science             NaN             29.0   \n",
              "...                          ...             ...              ...   \n",
              "1995            Computer Science            38.0             33.0   \n",
              "1996            Computer Science           113.0             17.0   \n",
              "1997            Computer Science             8.0             24.0   \n",
              "1998            Computer Science           430.0             21.0   \n",
              "1999                     Biology             3.0             37.0   \n",
              "\n",
              "                                             References  \n",
              "0     [4fb18a3560, ca52ece708, 902380a2c8, f62052d32...  \n",
              "1     [652660ddde, 1594e0e992, 215b4a9240, 704d2f26e...  \n",
              "2     [d42fcc494c, 7b3cc9650e, e33498b44d, a186029ed...  \n",
              "3     [8112adf21e, 7cb4a1caf7, b2e2f57a1f, 28742d657...  \n",
              "4     [b24bc35766, e56eb14c9e, 06615485a2, d67363cd3...  \n",
              "...                                                 ...  \n",
              "1995  [c8849eb772, 352abe1fbc, e6415c4df3, 2c2f03071...  \n",
              "1996  [8936ea1d26, cd5aee3fc8, c8849eb772, 124d5a1a7...  \n",
              "1997  [8936ea1d26, f10d491a1a, 1c984a9afe, c8849eb77...  \n",
              "1998  [4d75cbd1aa, 7150643504, 276859caf3, b8482011f...  \n",
              "1999  [1526ccd81a, 124d5a1a7b, 773a726261, 58d000b90...  \n",
              "\n",
              "[2000 rows x 10 columns]"
            ]
          },
          "execution_count": 272,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with open('./content/MyDrive/paper_folder/crawled_paper_profName.json') as p:\n",
        "  paper = json.load(p)\n",
        "\n",
        "papers = pd.DataFrame(paper)\n",
        "papers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4l4iInfJeNp"
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "<font face=\"XB Zar\" size=4>\n",
        "    <h1>\n",
        "    <b>PageRank\n",
        "        شخصی‌سازی‌شده\n",
        "        (۲۰ نمره)</b>\n",
        "    </h1>\n",
        "</font>\n",
        "    <br>\n",
        "<font face=\"XB Zar\" size=3>\n",
        "در این بخش، الگوریتم\n",
        "    PageRank\n",
        "    شخصی‌سازی‌شده را پیاده‌سازی می‌کنیم که توسعه‌ای از الگوریتم\n",
        "    PageRank\n",
        "    است که ترجیحات کاربر را در نظر می‌گیرد. الگوریتم\n",
        "    PageRank\n",
        "    شخصی‌سازی‌شده گره‌ها را در یک گراف بر اساس اهمیت آنها برای کاربر رتبه‌بندی می‌کند، نه بر اساس اهمیت کلی آنها در نمودار.\n",
        "\n",
        "</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bBQ54igJeNp"
      },
      "outputs": [],
      "source": [
        "def pagerank(graph: Dict[str, List[str]], personalization: Dict[str, int]) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Returns the personalized PageRank scores for the nodes in the graph, given the user's preferences.\n",
        "\n",
        "    Parameters:\n",
        "    graph (Dict[str, List[str]]): The graph represented as a dictionary of node IDs and their outgoing edges.\n",
        "\n",
        "    Returns:\n",
        "    Dict[str, float]: A dictionary of node IDs and their personalized PageRank scores.\n",
        "    \"\"\"\n",
        "\n",
        "    id = np.array(list(papers.ID))\n",
        "    references = np.array(sum(list(papers.References), []))\n",
        "    all_papers = set(np.append(id, references))\n",
        "\n",
        "    graph_id2paper = {i: k for i, k in enumerate(all_papers)}\n",
        "    graph_paper2id = {k: i for i, k in enumerate(all_papers)}\n",
        "\n",
        "    n = len(graph_paper2id)\n",
        "\n",
        "    alpha = 0.85\n",
        "\n",
        "    P = np.zeros((n, n))\n",
        "    for i, (k, v) in enumerate(graph.items()):\n",
        "        for node in v:\n",
        "            id = graph_paper2id[k]\n",
        "            P[id][graph_paper2id[node]] = 1\n",
        "\n",
        "    # make every row sum-up to 1\n",
        "    S = P.sum(axis=1)\n",
        "    S[S==0] = 1\n",
        "    P = np.divide(P.T, S).T\n",
        "\n",
        "    v = np.ones(n)\n",
        "    P = (1 - alpha) * P + alpha * v / n\n",
        "\n",
        "\n",
        "    pr = np.ones(n) / n # initialize pagerank vector with equal probability\n",
        "    preferences = np.zeros(n)\n",
        "    for k, v in personalization.items():\n",
        "        preferences[graph_paper2id[k]] = v\n",
        "\n",
        "    converged = False\n",
        "    tol = 1e-5\n",
        "    while not converged:\n",
        "        pr_next = (1 - alpha) * np.dot(P, pr) + alpha * preferences # update pagerank vector\n",
        "        if np.linalg.norm(pr_next - pr) < tol: # check for convergence\n",
        "            converged = True\n",
        "        pr = pr_next\n",
        "\n",
        "\n",
        "    pr_dict = {}\n",
        "    for i in range(n):\n",
        "        pr_dict[graph_id2paper[i]] = pr[i]\n",
        "\n",
        "    return pr_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJ5Cr_emYytJ"
      },
      "outputs": [],
      "source": [
        "prof_papers = ['78784ff663', '78784ff663', '78784ff663', '1c3f9efb66', '62b5d4d3fe',\n",
        "               '6118586740', 'f6b26f1c77', '76440d3df8', 'f7fa0fae40e82', 'ae5a4ebb97',\n",
        "               '0a6a6775bf', '8b34e67f7d', 'b4c6b174f5', '2005407acf', '3682775136',\n",
        "               '80c8843558', '8b34e67f7d', '74b74e9c7a', '527cd9fa95', 'cafc00f9da',\n",
        "               '7d3ebdfd95', '03b2953d52', 'b7b17f9a73', '1cc2dfad5f', '14063923e4']\n",
        "\n",
        "personalization = {node: 1 for node in papers[papers.ID.isin(prof_papers)].ID}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHk5Osz_uzFc"
      },
      "outputs": [],
      "source": [
        "graph = {}\n",
        "for ind, row in papers.iterrows():\n",
        "    graph[row[\"ID\"]] = row[\"References\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naEJbGOzuxw9"
      },
      "outputs": [],
      "source": [
        "pr = pagerank(graph, personalization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvCwo3tvJeNq"
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "<font face=\"XB Zar\" size=3>\n",
        "در این بخش از الگوریتم\n",
        "PageRank\n",
        "شخصی‌سازی‌شده که در قسمت قبلی پیاده‌سازی شده‌است برای\n",
        "شناسایی مقالات مهم مرتبط با حوزه‌ی کاری یک استاد\n",
        "خاص استفاده می‌کنیم. این تابع، یک\n",
        "    field\n",
        "    را به عنوان ورودی دریافت می‌کند. خروجی نیز\n",
        "مقالات برتری که بیشترین ارتباط را با آن زمینه دارند؛ خواهدبود.\n",
        "</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfq1i_W3JeNq"
      },
      "outputs": [],
      "source": [
        "def important_articles(Professor: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Returns the most important articles in the field of given professor, based on the personalized PageRank scores.\n",
        "\n",
        "    Parameters:\n",
        "    Professor (str): Professor's name.\n",
        "\n",
        "    Returns:\n",
        "    List[str]: A list of article IDs representing the most important articles in the field of given professor.\n",
        "    \"\"\"\n",
        "\n",
        "    exist = papers['Authors'].str.find(Professor)\n",
        "    paper_ids = papers['ID'][exist != -1]\n",
        "\n",
        "    prof_papers = {}\n",
        "    for id in paper_ids:\n",
        "        prof_papers[id] = pr[id]\n",
        "\n",
        "    ordered = sorted(prof_papers.items(), key=lambda x: x[1], reverse=True)\n",
        "    prof_papers = [p[0] for p in ordered]\n",
        "\n",
        "    return prof_papers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C48R6A8opCw-",
        "outputId": "a410897f-85f3-474e-fdae-660782f0786d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['7d3ebdfd95',\n",
              " '03b2953d52',\n",
              " 'b7b17f9a73',\n",
              " '1cc2dfad5f',\n",
              " '14063923e4',\n",
              " 'dc5b8619b1',\n",
              " '9b46be4df5',\n",
              " '2cd3f69cd2']"
            ]
          },
          "metadata": {},
          "execution_count": 315
        }
      ],
      "source": [
        "important_articles('Kasaei')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ib4e4OVJJeNr"
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "<font face=\"XB Zar\" size=4>\n",
        "    <h1>\n",
        "    <b>جستجو شخصی‌سازی‌شده (۱۰ نمره)</b>\n",
        "    </h1>\n",
        "</font>\n",
        "    <br>\n",
        "<font face=\"XB Zar\" size=3>\n",
        "الگوریتم جست‌و‌جویی که در فازهای گذشته پیاده‌سازی کرده‌اید را به گونه‌ای تغییر دهید که نتایج به دست آمده جست‌و‌جو بر حسب علایق فرد مرتب شوند. از قضیه‌ی خطی بودن برای این کار استفاده کنید.\n",
        "\n",
        "</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hazm"
      ],
      "metadata": {
        "id": "54xC4cZu-ivW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from hazm import word_tokenize\n",
        "import string\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "eng_stopwords = nltk.corpus.stopwords.words('english')"
      ],
      "metadata": {
        "id": "T42_c65w-YOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text, minimum_length=1, stopword_removal=True, stopwords_domain=[], lower_case=True,\n",
        "                       punctuation_removal=True):\n",
        "    # todo\n",
        "    if punctuation_removal:\n",
        "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "    normalized_tokens = []\n",
        "    for token in tokens:\n",
        "        token = str(token)\n",
        "        if lower_case:\n",
        "            token = token.lower()\n",
        "\n",
        "        if stopword_removal:\n",
        "            if (token in stopwords_domain or token in eng_stopwords):\n",
        "                continue\n",
        "\n",
        "        token = re.sub(r\"\\s+\", '', token)\n",
        "        token = re.sub(r\"\\d\", '', token)\n",
        "        if token != '':\n",
        "            normalized_tokens.append(token)\n",
        "\n",
        "    return normalized_tokens"
      ],
      "metadata": {
        "id": "dg5xoVMy-BQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "papers['preprocessed_abstract'] = papers['Abstract'].apply(lambda x: preprocess_text(x))\n",
        "papers['preprocessed_title'] = papers['Title'].apply(lambda x: preprocess_text(x))"
      ],
      "metadata": {
        "id": "f5v5Ocp9_CBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJMYcs2L32oS"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def add_word_to_dict(doc, word, docid, pos, col):\n",
        "    if word in doc:\n",
        "        if docid in doc[word]:\n",
        "            if col in doc[word][docid]:\n",
        "                doc[word][docid][col].append(pos)\n",
        "            else:\n",
        "                doc[word][docid][col] = [pos]\n",
        "        else:\n",
        "            doc[word][docid] = {col: [pos]}\n",
        "    else:\n",
        "        doc[word] = {docid: {col: [pos]}}\n",
        "\n",
        "    return doc\n",
        "\n",
        "def construct_positional_indexes(corpus : str):\n",
        "\n",
        "    docs = dict()\n",
        "    for docid, row in corpus.iterrows():\n",
        "        for pos, word in enumerate(row['preprocessed_title']):\n",
        "            docs = add_word_to_dict(docs, word, row['ID'], pos, 'title')\n",
        "\n",
        "        for pos, word in enumerate(row['preprocessed_abstract']):\n",
        "            docs = add_word_to_dict(docs, word, row['ID'], pos, 'abstract')\n",
        "\n",
        "    return docs\n",
        "\n",
        "\n",
        "docs = construct_positional_indexes(papers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvUKND_o35Rs"
      },
      "outputs": [],
      "source": [
        "def calculate_idf():\n",
        "    DF = {}\n",
        "    for word, pos in docs.items():\n",
        "        DF[word] = {'title': 0, 'abstract':0}\n",
        "        N = len(papers.index)\n",
        "        for v in pos.values():\n",
        "            if v.get('title') is not None:\n",
        "                DF[word]['title'] += 1\n",
        "            if v.get('abstract') is not None:\n",
        "                DF[word]['abstract'] += 1\n",
        "        try:\n",
        "            DF[word]['title'] = np.log(N // DF[word]['title'])\n",
        "        except:\n",
        "            DF[word]['title'] = 0\n",
        "        try:\n",
        "            DF[word]['abstract'] = np.log(N // DF[word]['abstract'])\n",
        "        except:\n",
        "            DF[word]['abstract'] = 0\n",
        "\n",
        "    return DF\n",
        "\n",
        "\n",
        "def champion_list(query):\n",
        "    all_query_docs = []           # all the doc that have at least one query term\n",
        "    for w in query:\n",
        "        posting_list = list(docs[w].keys()) if docs.get(w) is not None else []\n",
        "        all_query_docs.extend(posting_list)\n",
        "\n",
        "    top_docs = Counter(all_query_docs).most_common()\n",
        "    max_doc = top_docs[0][1]\n",
        "\n",
        "    ind = 1\n",
        "    if max_doc == 1 or max_doc == 2:\n",
        "        ind = len(top_docs) - 1\n",
        "    else:\n",
        "        for i, (docid, repetition) in enumerate(top_docs):\n",
        "            if repetition <= max_doc // 2:\n",
        "                ind = i\n",
        "                break\n",
        "\n",
        "    high, low = list(list(zip(*top_docs[:ind]))[0]), list(list(zip(*top_docs[ind:]))[0])\n",
        "    return high, low"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkapbIcN381J"
      },
      "outputs": [],
      "source": [
        "def search_vector_space(candidates, mode, title_query, abstract_query, query_title_tf, query_abstract_tf, idf, weight):\n",
        "    doc_scores = {}\n",
        "    for docid, row in candidates.iterrows():\n",
        "        # Document customization base on mode\n",
        "        doc_title_vector = {word: 1 + np.log(raw_tf) for word, raw_tf in Counter(row['preprocessed_title']).items()}\n",
        "        doc_abstract_vector = {word: 1 + np.log(raw_tf) for word, raw_tf in Counter(row['preprocessed_abstract']).items()}\n",
        "\n",
        "        if mode[1] == 't':          # doc shoud have idf\n",
        "            doc_title_vector = {word: tf * idf[word]['title'] for word, tf in doc_title_vector.items()}\n",
        "            doc_abstract_vector = {word: tf * idf[word]['abstract'] for word, tf in doc_abstract_vector.items()}\n",
        "\n",
        "        if mode[2] == 'c':         # doc need length normalization\n",
        "            len_norm = np.sqrt(np.sum(np.array(list(doc_title_vector.values())) ** 2))\n",
        "            doc_title_vector = {word: tf / len_norm for word, tf in doc_title_vector.items()}\n",
        "\n",
        "            len_norm = np.sqrt(np.sum(np.array(list(doc_abstract_vector.values())) ** 2))\n",
        "            doc_abstract_vector = {word: tf / len_norm for word, tf in doc_abstract_vector.items()}\n",
        "\n",
        "        # Query customization base on mode\n",
        "        query_title_vector = {word: 1 + np.log(raw_tf) for word, raw_tf in query_title_tf.items()}\n",
        "        query_abstract_vector = {word: 1 + np.log(raw_tf) for word, raw_tf in query_abstract_tf.items()}\n",
        "\n",
        "        if mode[5] == 't':\n",
        "            query_title_vector = {word: tf * idf[word]['title'] if idf.get(word) is not None else 0\n",
        "                                  for word, tf in query_title_vector.items()}\n",
        "            query_abstract_vector = {word: tf * idf[word]['abstract'] if idf.get(word) is not None else 0\n",
        "                                     for word, tf in query_abstract_vector.items()}\n",
        "\n",
        "        if mode[6] == 'c':\n",
        "            len_norm = np.sqrt(np.sum(np.array(list(query_title_vector.values())) ** 2))\n",
        "            query_title_vector = {word: tf / len_norm for word, tf in query_title_vector.items()}\n",
        "\n",
        "            len_norm = np.sqrt(np.sum(np.array(list(query_abstract_vector.values())) ** 2))\n",
        "            query_abstract_vector = {word: tf / len_norm for word, tf in query_abstract_vector.items()}\n",
        "\n",
        "        def calculate_score(query, doc_vector, query_vector):\n",
        "            score = 0\n",
        "            for word in query:\n",
        "                q = query_vector.get(word) if query_vector.get(word) is not None else 0\n",
        "                d = doc_vector.get(word) if doc_vector.get(word) is not None else 0\n",
        "                score += q * d\n",
        "\n",
        "            return score\n",
        "\n",
        "        title_score = calculate_score(title_query, doc_title_vector, query_title_vector)\n",
        "        abstract_score = calculate_score(abstract_query, doc_abstract_vector, query_abstract_vector)\n",
        "\n",
        "        doc_scores[row['ID']] = weight * abstract_score + (1 - weight) * title_score\n",
        "    return doc_scores\n",
        "\n",
        "\n",
        "def search_bm25(candidates, title_query, abstract_query, idf, k, b, weight):\n",
        "    avg_title_doc_len = np.sum(list(Counter(list(np.concatenate(candidates.title.tolist()))).values())) // len(candidates.title.index)\n",
        "    avg_abstract_doc_len = np.sum(list(Counter(list(np.concatenate(candidates.abstract.tolist()))).values())) // len(candidates.abstract.index)\n",
        "\n",
        "    doc_scores = {}\n",
        "    for docid, row in candidates.iterrows():\n",
        "\n",
        "        def calculate_score(query, doc_tf, avg_doc_len, doc_type):\n",
        "            doc_len = np.sum(list(doc_tf.values()))\n",
        "            doc_normalize = (1-b + b*(doc_len/avg_doc_len))\n",
        "\n",
        "            score = 0\n",
        "            for word in query:\n",
        "                query_idf = idf[word][doc_type] if idf.get(word) is not None else 0\n",
        "                tf = doc_tf.get(word) if doc_tf.get(word) is not None else 0\n",
        "\n",
        "                c = query_idf * (((k + 1) * tf) // (k * doc_normalize + tf))\n",
        "                score += c\n",
        "\n",
        "            return score\n",
        "\n",
        "        doc_title_tf = Counter(row['preprocessed_title'])\n",
        "        doc_abstract_tf = Counter(row['preprocessed_abstract'])\n",
        "\n",
        "        title_score = calculate_score(title_query, doc_title_tf, avg_title_doc_len, 'title')\n",
        "        abstract_score = calculate_score(abstract_query, doc_abstract_tf, avg_abstract_doc_len, 'abstract')\n",
        "\n",
        "        doc_scores[row['ID']] = weight * abstract_score + (1 - weight) * title_score\n",
        "    return doc_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJDnkebQJeNr"
      },
      "outputs": [],
      "source": [
        "from typing import List, Tuple\n",
        "\n",
        "\n",
        "\n",
        "def search(title_query: str, abstract_query: str, max_result_count: int, method: str = 'ltn-lnn', weight: float = 0.5,\n",
        "           print_=False, preferred_field: str = None):\n",
        "    \"\"\"\n",
        "        Finds relevant documents to query\n",
        "\n",
        "        Parameters\n",
        "        ---------------------------------------------------------------------------------------------------\n",
        "        max_result_count: Return top 'max_result_count' docs which have the highest scores.\n",
        "                          notice that if max_result_count = -1, then you have to return all docs\n",
        "\n",
        "        mode: 'detailed' for searching in title and text separately.\n",
        "              'overall' for all words, and weighted by where the word appears on.\n",
        "\n",
        "        where: when mode ='detailed', when we want search query\n",
        "                in title or text not both of them at the same time.\n",
        "\n",
        "        method: 'ltn-lnn' or 'ltc-lnc' or 'okapi25'\n",
        "\n",
        "        preferred_field: A list containing preference rate to Dr. Rabiee, Dr. Soleymani, Dr. Rohban,\n",
        "                         Dr. Kasaei, and Dr. Sharifi's papers, respectively.\n",
        "\n",
        "        Returns\n",
        "        ----------------------------------------------------------------------------------------------------\n",
        "        list\n",
        "        Retrieved documents with snippet\n",
        "    \"\"\"\n",
        "    # TODO: retun top 'max_result_count' documents for your searched query\n",
        "\n",
        "    idf = calculate_idf()\n",
        "\n",
        "    # title Query\n",
        "    title_query = preprocess_text(title_query)\n",
        "    query_title_tf = Counter(title_query)\n",
        "\n",
        "    # abstract Query\n",
        "    abstract_query = preprocess_text(abstract_query)\n",
        "    query_abstract_tf = Counter(abstract_query)\n",
        "\n",
        "\n",
        "    high_docs, low_docs = champion_list(title_query + abstract_query)\n",
        "    high_candidates = papers[papers.ID.apply(lambda r: r in high_docs)]\n",
        "    low_candidates = papers[papers.ID.apply(lambda r: r in low_docs)]\n",
        "\n",
        "    result = []\n",
        "    doc_scores = {}\n",
        "\n",
        "    if method[0] == 'l':  # for all 'ltn-lnn' or 'ltc-lnc' or 'lnc-ltn'\n",
        "        doc_scores = search_vector_space(high_candidates, method, title_query, abstract_query, query_title_tf, query_abstract_tf, idf, weight)\n",
        "\n",
        "        if len(doc_scores) < max_result_count or max_result_count == -1:\n",
        "            low_docs = search_vector_space(low_candidates, method, title_query, abstract_query, query_title_tf, query_abstract_tf, idf, weight)\n",
        "            doc_scores.update(low_docs)\n",
        "\n",
        "        doc_scores = sorted(doc_scores.items(), key=lambda x:x[1], reverse=True)\n",
        "        result = list(list(zip(*doc_scores[:max_result_count]))[0])\n",
        "\n",
        "    elif method == 'okapi25':\n",
        "        k = 4\n",
        "        b = .6\n",
        "        doc_scores = search_bm25(high_candidates, title_query, abstract_query, idf, k, b, weight)\n",
        "\n",
        "        if len(doc_scores) < max_result_count or max_result_count == -1:\n",
        "            low_docs =  search_bm25(low_candidates, title_query, abstract_query, idf, k, b, weight)\n",
        "            doc_scores.update(low_docs)\n",
        "\n",
        "        doc_scores = sorted(doc_scores.items(), key=lambda x:x[1], reverse=True)\n",
        "        result = list(list(zip(*doc_scores[:max_result_count]))[0])\n",
        "\n",
        "    new_pr = personal_pagerank('Rabiee', preferred_field[0], pr)\n",
        "    new_pr = personal_pagerank('Soleymani', preferred_field[0], new_pr)\n",
        "    new_pr = personal_pagerank('Rohban', preferred_field[0], new_pr)\n",
        "    new_pr = personal_pagerank('Kasaei', preferred_field[0], new_pr)\n",
        "    new_pr = personal_pagerank('Sharifi', preferred_field[0], new_pr)\n",
        "\n",
        "    result_dict = {new_pr[r]: r for r in result}\n",
        "    result_dict = sorted(result_dict.items(), reverse=True)\n",
        "    result = [r[1] for r in result_dict]\n",
        "\n",
        "    if print_ == True:\n",
        "        res_df = papers[papers.index.isin(result)]\n",
        "        for d_id, row in res_df.iterrows():\n",
        "            print(row[['ID', 'preprocessed_title', 'preprocessed_abstract']])\n",
        "            print()\n",
        "\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def personal_pagerank(prof, weight, old_pr):\n",
        "  new_pagerank = old_pr.copy()\n",
        "  prof_papers = important_articles(prof)\n",
        "  for paper in prof_papers:\n",
        "    new_pagerank[paper] *= weight\n",
        "\n",
        "  return new_pagerank"
      ],
      "metadata": {
        "id": "-iyiOWSWPfPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = search('GAN image', 'vision', 10, preferred_field=[0.1, 0.2, 0.3, 0.3, 0.1])\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8qV6spQAJ5V",
        "outputId": "06a01e54-ab95-4958-a108-7a765221f244"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ebb2459f7e',\n",
              " 'c5f9744852',\n",
              " '8048010873',\n",
              " '30fbfe6ecf',\n",
              " 'f89529dbce',\n",
              " '34a0690007',\n",
              " '34d7a8cf30',\n",
              " '4fcc93e345',\n",
              " '09c51cbc67',\n",
              " '6138678cf5']"
            ]
          },
          "metadata": {},
          "execution_count": 338
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFlgSJD-JeNr"
      },
      "source": [
        "<div dir=\"rtl\" style=\"text-align: justify\">\n",
        "<font face=\"XB Zar\" size=4>\n",
        "    <h1>\n",
        "    <b>رتبه‌بندی نویسندگان (۲۵ نمره)</b>\n",
        "    </h1>\n",
        "</font>\n",
        "    <br>\n",
        "<font face=\"XB Zar\" size=3>\n",
        "    برای رتبه‌بندی نویسندگان، مفهوم ارجاع نویسندگان به یکدیگر مطرح می‌شود. زمانی که نویسنده A در مقاله خود به مقاله P که نویسنده B جزو نویسندگان آن مقاله یعنی مقاله P می‌باشد، ارجاع دهد، می‌گوییم که نویسنده A به نویسنده B ارجاع داده است. با توجه به این رابطه، می‌توان گراف ارجاعات بین نویسندگان را ایجاد و سپس با استفاده از الگوریتم HITS\n",
        "نویسندگان را رتبه‌بندی کرد. برای رتبه‌بندی نیاز است تا از شاخص‌های hub و authority استفاده کنیم.\n",
        "\n",
        "\n",
        "</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpAkJJvpJeNr",
        "outputId": "d6e0560a-04d5-474b-fa83-85b5eb82b074"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Jiri Matas', 'M. Kristan', 'A. Leonardis', 'G. Fernandez', 'M. Felsberg']\n"
          ]
        }
      ],
      "source": [
        "import networkx as nx\n",
        "import re\n",
        "import json\n",
        "\n",
        "def hit_algorithm(papers, n):\n",
        "    \"\"\"\n",
        "        Implementing the HITS algorithm to score authors based on their papers and co-authors.\n",
        "\n",
        "        Parameters\n",
        "        ---------------------------------------------------------------------------------------------------\n",
        "        papers: A list of paper dictionaries with the following keys:\n",
        "                \"id\": A unique ID for the paper\n",
        "                \"title\": The title of the paper\n",
        "                \"abstract\": The abstract of the paper\n",
        "                \"date\": The year in which the paper was published\n",
        "                \"authors\": A list of the names of the authors of the paper\n",
        "                \"related_topics\": A list of IDs for related topics (optional)\n",
        "                \"citation_count\": The number of times the paper has been cited (optional)\n",
        "                \"reference_count\": The number of references in the paper (optional)\n",
        "                \"references\": A list of IDs for papers that are cited in the paper (optional)\n",
        "        n: An integer representing the number of top authors to return.\n",
        "\n",
        "        Returns\n",
        "        ---------------------------------------------------------------------------------------------------\n",
        "        List\n",
        "        list of the top n authors based on their hub scores.\n",
        "    \"\"\"\n",
        "    # Create a graph of authors and papers (all of the authors and papers represented as nodes, and all of the authors who wrote each paper connected to the corresponding paper node by an edge)\n",
        "\n",
        "    g = {}\n",
        "    for p in papers:\n",
        "        authors = p['Authors'].split(', ')\n",
        "        g[p[\"ID\"]] = authors\n",
        "\n",
        "    G = nx.Graph(g)\n",
        "\n",
        "    # Run the HITS algorithm\n",
        "    hubs, authorities = nx.hits(G)\n",
        "\n",
        "    # Create a list of top n authors based on their hub scores\n",
        "    authors = {}\n",
        "    for k, v in hubs.items():\n",
        "      if len(re.findall(r'\\d+', k)) == 0:\n",
        "          authors[k] = v\n",
        "\n",
        "    top_authors = [prof[0] for prof in sorted(authors.items(), key=lambda x:x[1], reverse=True)]\n",
        "    if n < len(top_authors):\n",
        "      return top_authors[:n]\n",
        "\n",
        "    return top_authors\n",
        "\n",
        "\n",
        "\n",
        "# call the hit_algorithm function\n",
        "top_authors = hit_algorithm(paper, 5)\n",
        "\n",
        "# print the top authors\n",
        "print(top_authors)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_z_-3awJeNr"
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "<font face=\"XB Zar\" size=4>\n",
        "    <h1>\n",
        "    <b>سیستم پیشنهادگر (۲۰ نمره)</b>\n",
        "    </h1>\n",
        "</font>\n",
        "<font face=\"XB Zar\" size=3>\n",
        "\n",
        "در این بخش سعی می‌کنیم که یک سیستم پیشنهادگر مقالات بر اساس جست‌و‌جو‌ها یا علايق یک کاربر پیاده‌سازی کنیم، سیستم پیشنهاد دهنده‌ای که قصد داریم آن را ایجاد کنیم،‌ باید بتواند بر اساس لیستی از مقالاتی که کاربر قبلا آن‌ها را مطالعه کرده یا به آن‌ها علاقه داشته است، مقالات تازه انتشار یافته‌‌ی جدید را به کاربر پیشنهاد دهد.\n",
        "\n",
        "در فایل recommended_papers.json\n",
        "لیستی از کاربران قرار دارد که در فیلد positive_papers هر کاربر،\n",
        "تعداد ۵۰ مقاله از مقالاتی که کاربر به آن‌ها علاقه داشته است مشخص شده است. و همچینین در فیلد recommendedPapers هر کاربر تعداد ۱۰ مقاله به ترتیب اهمیت، از مقالات جدیدی که کاربر آن‌ها را پسندیده است قرار دارد.\n",
        "\n",
        "در این بخش هدف شما یادگیری سیستم پیشنهاد‌ دهنده بر اساس همین داده‌ها می‌باشد، و به عبارتی شما بایستی کاربر‌ها را به دو دسته آموزش و آزمایش تقسیم کنید، و بر اساس داده‌های آموزشی بتوانید مقالات جدید مورد پسند کاربرهای آزمایش را پیش‌بینی کنید. (بنابراین در این پیش‌بینی نمی‌توانید از فیلد recommendedPapers این کاربران استفاده کنید.)\n",
        "\n",
        "</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8B7RNB8JeNs"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open('./content/MyDrive/paper_folder/recommended_papers.json', 'r') as fp:\n",
        "    recommended_papers = json.load(fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLLmZ_FPJeNs"
      },
      "outputs": [],
      "source": [
        "sample_user = recommended_papers[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFStPx_eJeNs",
        "outputId": "7dfcb2f5-c368-42b3-be2a-732d507c9e2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "d9404b4a794c07b5e2cdf3203aabf06d70c6be9b\n",
            "CENTAURO: A Hybrid Locomotion and High Power Resilient Manipulation Platform\n",
            "Despite the development of a large number of mobile manipulation robots, very few platforms can demonstrate the required strength and mechanical sturdiness to accommodate the needs of real-world applications with high payload and moderate/harsh physical interaction demands, e.g., in disaster-response scenarios or heavy logistics/collaborative tasks. In this letter, we introduce the design of a wheeled-legged mobile manipulation platform capable of executing demanding manipulation tasks, and demonstrating significant physical resilience while possessing a body size (height/width) and weight compatible to that of a human. The achieved performance is the result of combining a number of design and implementation principles related to the actuation system, the integration of body structure and actuation, and the wheeled-legged mobility concept. These design principles are discussed, and the solutions adopted for various robot components are detailed. Finally, the robot performance is demonstrated in a set of experiments validating its power and strength capability when manipulating heavy payload and executing tasks involving high impact physical interactions.\n",
            "['Computer Science']\n"
          ]
        }
      ],
      "source": [
        "print(sample_user['positive_papers'][0]['paperId'])\n",
        "print(sample_user['positive_papers'][0]['title'])\n",
        "print(sample_user['positive_papers'][0]['abstract'])\n",
        "print(sample_user['positive_papers'][0]['fieldsOfStudy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiYEklQNJeNs",
        "outputId": "56bf29bb-c265-4a0e-c7e4-44e111780434"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "94eebbefe8a37cf394be899b85af295c2e3a1f01\n",
            "Efficient Parametric Approximations of Neural Network Function Space Distance\n",
            "It is often useful to compactly summarize important properties of model parameters and training data so that they can be used later without storing and/or iterating over the entire dataset. As a specific case, we consider estimating the Function Space Distance (FSD) over a training set, i.e. the average discrepancy between the outputs of two neural networks. We propose a Linearized Activation Function TRick (LAFTR) and derive an efficient approximation to FSD for ReLU neural networks. The key idea is to approximate the architecture as a linear network with stochastic gating. Despite requiring only one parameter per unit of the network, our approach outcompetes other parametric approximations with larger memory requirements. Applied to continual learning, our parametric approximation is competitive with state-of-the-art nonparametric approximations, which require storing many training examples. Furthermore, we show its efficacy in estimating influence functions accurately and detecting mislabeled examples without expensive iterations over the entire dataset.\n",
            "['Computer Science', 'Mathematics']\n"
          ]
        }
      ],
      "source": [
        "print(sample_user['recommendedPapers'][0]['paperId'])\n",
        "print(sample_user['recommendedPapers'][0]['title'])\n",
        "print(sample_user['recommendedPapers'][0]['abstract'])\n",
        "print(sample_user['recommendedPapers'][0]['fieldsOfStudy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHj1bkY0moxa"
      },
      "outputs": [],
      "source": [
        "train_data, test_data = recommended_papers[:450], recommended_papers[450:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVFGe4PzJeNs"
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "<font face=\"XB Zar\" size=4>\n",
        "    <h2>\n",
        "    <b>روش Collaborative Filtering (۱۰ نمره)</b>\n",
        "    </h2>\n",
        "</font>\n",
        "<font face=\"XB Zar\" size=3>\n",
        "\n",
        "در این راهکار سعی می‌کنیم با استفاده از کاربران مشابه با یک کاربر، سلیقه‌ی او را حدس بزنیم و مقالاتی را که کاربران مشابه دیده‌اند را به کاربر نمایش دهیم.\n",
        "\n",
        "در این روش ابتدا باید $N$ کاربر که سلیقه‌ی مشابه با کاربر $x$ دارند را پیدا کنید، و با ترکیب لیست مقالات جدید مورد علاقه‌ی آن $N$ کاربر مشابه،\n",
        " ۱۰ مقاله‌ به کاربر $x$ پیشنهاد دهید.\n",
        "\n",
        "توجه داشته باشید که برای اینکه شباهت دو کاربر را پیدا کنید، باید cosine_similarity بین بردار زمینه‌های مورد علاقه‌ی دو کاربر استفاده کنید. این بردار از $M$ درایه تشکیل شده است، که $M$ تعداد زمینه‌های یکتاییست که در داده‌ها وجود دارد. و در این بردار درایه‌ی $j$ام\n",
        "نشان دهنده‌ی نسبت تعداد مقالات خوانده‌ی شده‌ کاربر در زمینه‌ی $j$ به تعداد کل مقاله‌های خوانده شده توسط او می‌باشد. (توجه کنید که هر مقاله می‌تواند چند زمینه داشته باشد و بنابراین حاصل جمع درایه‌های این بردار الزاما یک نمی‌باشد)\n",
        "\n",
        "</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oO01w99kJeNt"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def collaborative_filtering(user_id: int, N=10):\n",
        "    \"\"\"\n",
        "    Returns the top 10 related articles to the user, based on similar users (Similar users should be on \"train data\").\n",
        "\n",
        "    Parameters:\n",
        "    user_id (int): The unique index of the user.\n",
        "    N: The number of hyperparameter N in Nearest Neighbor algorithm.\n",
        "\n",
        "    Returns:\n",
        "    List[str]: A list of 10 article IDs that are most relevant to the user's interests.\n",
        "    \"\"\"\n",
        "    # TODO\n",
        "\n",
        "    all_fields = []\n",
        "    for user in train_data:\n",
        "      for item in user['positive_papers']:\n",
        "        if item['fieldsOfStudy'] is not None:\n",
        "          all_fields.extend(item['fieldsOfStudy'])\n",
        "\n",
        "    all_fields_dict = {f: 0 for f in set(all_fields)}\n",
        "\n",
        "    # all train users embeddings => 450 * 18\n",
        "    users_similarities = list()\n",
        "    for user in train_data:\n",
        "        user_fields = all_fields_dict.copy()\n",
        "        for item in user['positive_papers']:\n",
        "            if item['fieldsOfStudy'] is not None:\n",
        "                for field in item['fieldsOfStudy']:\n",
        "                  user_fields[field] += 1\n",
        "\n",
        "        users_similarities.append(list(np.array(list(user_fields.values())) / len(user['positive_papers'])))\n",
        "\n",
        "    users_similarities = np.array(users_similarities)\n",
        "\n",
        "    # the user embedding from fields of studies on positive papers => 1* 18\n",
        "    for item in recommended_papers[user_id]['positive_papers']:\n",
        "        user_fields = all_fields_dict.copy()\n",
        "        if item['fieldsOfStudy'] is not None:\n",
        "            for field in item['fieldsOfStudy']:\n",
        "                user_fields[field] += 1\n",
        "\n",
        "    user_embedding = np.array(list(np.array(list(user_fields.values())) / len(user['positive_papers']))).reshape(1, -1)\n",
        "\n",
        "    # similarity of all_users with the users => 450 * 1\n",
        "    X = cosine_similarity(users_similarities, user_embedding)\n",
        "\n",
        "    # find top N similar users\n",
        "    similar_users = X.flatten().argsort()[::-1][:N]\n",
        "\n",
        "    paper_recommended = []\n",
        "    for user in similar_users:\n",
        "      paper_recommended.append(recommended_papers[user]['recommendedPapers'][0]['paperId'])\n",
        "\n",
        "    return paper_recommended"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYenQrbc-kRy",
        "outputId": "bd343ae0-75f8-4ce3-e925-7b951e35e84b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['5cbadc7545b5296a8b245be20c78f8b9b628973c',\n",
              " '590b8f5e17424d1d4be560a0d2b1c665d8d3c7f8',\n",
              " 'b65dfaf9b95b21840848b3b77bb4df655305ac89',\n",
              " '2714c11c0809d638e1e501831913671914407e5d',\n",
              " 'ba852c774c00894376bc20cdccb884b0dbe1196b',\n",
              " '590b8f5e17424d1d4be560a0d2b1c665d8d3c7f8',\n",
              " '2714c11c0809d638e1e501831913671914407e5d',\n",
              " '2714c11c0809d638e1e501831913671914407e5d',\n",
              " '2714c11c0809d638e1e501831913671914407e5d',\n",
              " 'ba852c774c00894376bc20cdccb884b0dbe1196b']"
            ]
          },
          "execution_count": 246,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "collaborative_filtering(450, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-jqD-dkJeNt"
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "<font face=\"XB Zar\" size=4>\n",
        "    <h2>\n",
        "    <b>روش Content Based (۱۰ نمره)</b>\n",
        "    </h2>\n",
        "</font>\n",
        "<font face=\"XB Zar\" size=3>\n",
        "\n",
        "در این روش با استفاده از مقالات قبلی که کاربر آن‌ها را پسندیده است، به کاربر مقاله‌ی جدید پیشنهاد می‌دهیم.\n",
        "\n",
        "برای اینکار ابتدا تمام مقالات پیشنهاد شده برای تمام کاربرها را سر جمع کنید. (در واقع مدلی که پیاده‌سازی می‌کنید نباید بداند که به کدام کاربر چه مقالاتی پیشنهاد شده است)\n",
        "\n",
        "سپس بردار tf-idf برای تایتل هر یک از مقالات را ایجاد کنید، و میانگین بردار مقالات مورد علاقه‌ی هر فرد را با لیستی که از مقالات جدید سر جمع کردید مقایسه کنید و ۱۰ تا از شبیه‌ترین مقالات را خروجی دهید.\n",
        "\n",
        "</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUTtHuLMJeNt"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def content_based_recommendation(user_id):\n",
        "    \"\"\"\n",
        "    Returns the top 10 related articles to the user, based on the titles of the articles.\n",
        "\n",
        "    Parameters:\n",
        "    user_id (int): The unique index of the user.\n",
        "\n",
        "    Returns:\n",
        "    List[str]: A list of 10 article IDs that are most relevant to the user's interests.\n",
        "    \"\"\"\n",
        "    #TODO\n",
        "    vectorizer = TfidfVectorizer(min_df=2, ngram_range=(1,1), stop_words='english')\n",
        "\n",
        "    all_recommended_papers_titles = []\n",
        "    for user in train_data:\n",
        "      for paper in user['recommendedPapers']:\n",
        "          all_recommended_papers_titles.append(paper['title'])\n",
        "\n",
        "    X = vectorizer.fit_transform(all_recommended_papers_titles).toarray()\n",
        "\n",
        "    user_likes = []\n",
        "    for p in recommended_papers[user_id]['positive_papers']:\n",
        "      user_likes.append(p['title'])\n",
        "\n",
        "    user_preference = vectorizer.transform(user_likes)\n",
        "    user_preference = user_preference.toarray().mean(axis=0).reshape(1, -1)\n",
        "\n",
        "    similar_papers = cosine_similarity(X, user_preference)\n",
        "    similar_papers = similar_papers.flatten().argsort()[::-1]\n",
        "\n",
        "    paper_recommended = []\n",
        "    for paper_id in similar_papers:\n",
        "        p_id = train_data[paper_id//10]['recommendedPapers'][paper_id%10]['paperId']\n",
        "\n",
        "        # to avoid recommending same papers\n",
        "        if p_id not in paper_recommended:\n",
        "            paper_recommended.append(p_id)\n",
        "\n",
        "        if len(paper_recommended) >= 10:\n",
        "            break\n",
        "\n",
        "    return paper_recommended"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ya5NYY0XIddT"
      },
      "outputs": [],
      "source": [
        "x = content_based_recommendation(450)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1sKXE-fJeNt"
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "<font face=\"XB Zar\" size=4>\n",
        "    <h2>\n",
        "    <b>ارزیابی سیستم‌های پیشنهادگر</b>\n",
        "    </h2>\n",
        "</font>\n",
        "<font face=\"XB Zar\" size=3>\n",
        "\n",
        "در این بخش سیستم‌های پیشنهادگری را که پیاده کرده‌اید را با استفاده از معیار nDCG و با استفاده از دادگان واقعی از علایق کاربران نسبت به مقالات جدید ارزیابی کنید و نتایج حاصل از دو روش را با هم مقایسه کنید.\n",
        "\n",
        "</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Da1scztnPrWI"
      },
      "outputs": [],
      "source": [
        "def cacluate_NDCG(actual: List[List[str]], predicted: List[List[str]]) -> float:\n",
        "    # docs relevancy ranked from 10 to 1\n",
        "\n",
        "    ndcg = []\n",
        "    prefect_ranking =[10] + [i/np.log2(11 - i) for i in range(9,0,-1)]\n",
        "    prefect_ranking = np.sum(prefect_ranking)\n",
        "\n",
        "    # TODO: Calculate NDCG here\n",
        "    for user in range(len(actual)):\n",
        "        actual_ = actual[user]\n",
        "        pred = predicted[user]\n",
        "\n",
        "        dcg = []\n",
        "        for i in range(len(pred)):\n",
        "            if pred[i] in actual_:\n",
        "                if i == 0:\n",
        "                    dcg.append(10 - actual_.index(pred[i]))\n",
        "                else:\n",
        "                    dcg.append((10 - actual_.index(pred[i])) / np.log2(i+1))\n",
        "\n",
        "        dcg = np.round(np.array(dcg) / prefect_ranking, 3)\n",
        "        sum_ = np.round(np.sum(dcg), 3)\n",
        "        print(f'NDCG for {user}th query: {dcg} = {sum_}')\n",
        "\n",
        "        ndcg.append(sum_)\n",
        "\n",
        "    ndcg = np.round(np.mean(ndcg), 3)\n",
        "\n",
        "    return ndcg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wyz4qP1_JeNt"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "TODO: Compare two methods with nDCG metric.\n",
        "\"\"\"\n",
        "\n",
        "rec1 = []\n",
        "rec2 = []\n",
        "actual_rec = []\n",
        "\n",
        "for i in range(450,470):\n",
        "  rec1.append(collaborative_filtering(i, 10))\n",
        "  rec2.append(content_based_recommendation(i))\n",
        "\n",
        "  acc = []\n",
        "  for p in test_data[i%450]['recommendedPapers']:\n",
        "    acc.append(p['paperId'])\n",
        "\n",
        "  actual_rec.append(acc)\n",
        "\n",
        "print(f\"colaborative filtering score = {cacluate_NDCG(actual_rec, rec1)}\\n\")\n",
        "print(f\"content based score = {cacluate_NDCG(actual_rec, rec2)}\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "08ac30a6a1fd2e576b33e03f7d61c3a285d7ee0582c2dd23dde6343ef303ebe9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}